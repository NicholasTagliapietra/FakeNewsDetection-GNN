{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEmsKFyyGxHT"
   },
   "source": [
    "# **Basic GNN Model Hyperparameter Optimization**\n",
    "\n",
    "This notebook search for the best hyperparameters by implementing a Cross-Validation Setup. The Hyperparameters that will be searched are the Learning Rate, the Weight Decay and the Embedding Size of the Graph Neural Network.\n",
    "An Early Stopping Mechanism will help terminate trials that are not improving.\n",
    "\n",
    "The data from the UPFD Framework has been already split in Training, Validation and Test Set and is downloadable by using simple commands inside the Pytorch Geometric environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHmjRogXHXhz"
   },
   "source": [
    "## Download and import Libraries for the Environment \n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDPul_f5Lqfg"
   },
   "source": [
    "Download the right libraries depending if we are using a CPU or a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIWCzgikGwbH",
    "outputId": "11220e1d-b290-4d22-f788-c388408f2c3d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "  #!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.10.0+cu102.html\n",
    "#else:\n",
    "  #!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.10.0+cpu.html\n",
    "\n",
    "#!pip install optuna\n",
    "import optuna\n",
    "\n",
    "\n",
    "from torch.nn import Linear, LogSoftmax\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool, global_max_pool\n",
    "\n",
    "from tool_box.upfd_dataset import ext_UPFD\n",
    "from tool_box.GNN_train import train_step, val_step\n",
    "\n",
    "# Set GPU as Device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(f\"Training Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ8KZjaYSBKP"
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M9OEYmMfRwPP"
   },
   "outputs": [],
   "source": [
    "path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAKBZdnB9Hkt",
    "outputId": "a5f30164-46d6-4a57-dfbd-3c0d22c57f41"
   },
   "outputs": [],
   "source": [
    "train_dataset = ext_UPFD(name = 'original', root = path, n_features=[], g_features=[], split = 'train')\n",
    "val_dataset = ext_UPFD(name = 'original', root = path, n_features=[], g_features=[], split='val')\n",
    "test_dataset = ext_UPFD(name = 'original', root = path, n_features=[], g_features=[], split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gcpp_2zetD1x"
   },
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lCtlg09CtXVY"
   },
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_n_feature, num_g_feature, emb_size):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        self.in_layer = GCNConv(num_n_feature, emb_size)\n",
    "        self.conv1 = GCNConv(emb_size, emb_size)\n",
    "       \n",
    "       # check if we have graph features to concatenate or not\n",
    "        i = 2\n",
    "        if num_g_feature:\n",
    "             self.lin_g = Linear(num_g_feature, emb_size)\n",
    "             i = 3\n",
    "\n",
    "        self.out_layer = Linear(i * emb_size, 2)\n",
    "        self.act = LogSoftmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x, edges_idx, batch_idx, g_features):\n",
    "        #pdb.set_trace()\n",
    "        x = self.in_layer(x, edges_idx)\n",
    "\n",
    "        x = self.conv1(x, edges_idx)\n",
    "\n",
    "        flatten = torch.cat([global_mean_pool(x, batch_idx),\n",
    "                             global_max_pool(x, batch_idx)], axis=1)\n",
    "\n",
    "        if g_features.size()[-1]:\n",
    "            g_ft = self.lin_g(g_features)\n",
    "            flatten = torch.cat([flatten, g_ft], axis=1)\n",
    "\n",
    "        out = self.act(self.out_layer(flatten))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kkoEJHIvUD5L"
   },
   "outputs": [],
   "source": [
    "def train(trial,model,optimizer,loss_f, train_loader, val_loader, epochs = 60):\n",
    "    \n",
    "  acc_losses_t, acc_losses_v = [], []\n",
    "  acc_v = 999\n",
    "  for epoch in range(epochs):\n",
    "    loss_t, acc_t = train_step(model, train_loader, optimizer, loss_f)\n",
    "    loss_v, acc_v = val_step(model, val_loader, loss_f)\n",
    "        \n",
    "    acc_losses_t.append([loss_t, acc_t])\n",
    "    acc_losses_v.append([loss_v, acc_v])\n",
    "\n",
    "    trial.report(acc_v, epoch)\n",
    "\n",
    "    # Early Stopping\n",
    "    if trial.should_prune():\n",
    "      raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "  return acc_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy1kq27yuTh1"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ_D2AuKU-pa"
   },
   "source": [
    "Training Function that will be executed every time a trial is done. In this function the hyperparameters are chosen based on suggestions from the hyperparameter optimizer chosen with Optuna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gcRszH5TuZcI"
   },
   "outputs": [],
   "source": [
    "epochs_max = 60\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "  embedding_space_dim = trial.suggest_categorical(\"embedding_space_dim\",[40,60,80,100,120,140,160])\n",
    "  learning_rate = trial.suggest_categorical(\"learning_rate\",[0.001, 0.005, 0.01])\n",
    "  weight_decay = trial.suggest_categorical(\"weight_decay\",[0.001, 0.005, 0.01])\n",
    "\n",
    "  # If the trial has already been explored, prune it. It may happen because \n",
    "  # the hyperparameter optimizer searchs near the most promising values.\n",
    "  for t in trial.study.trials:\n",
    "    if t.state != optuna.trial.TrialState.COMPLETE:\n",
    "      continue\n",
    "    if t.params == trial.params:\n",
    "      raise optuna.exceptions.TrialPruned('Duplicate Parameter Set')\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "  val_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "  num_node_features = train_dataset.num_features\n",
    "  num_graph_features = train_dataset.g_features\n",
    "  model = GNN(num_n_feature = num_node_features, num_g_feature = num_graph_features, emb_size = embedding_space_dim).to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "  loss_f = torch.nn.NLLLoss()\n",
    "\n",
    "  return train(trial,model,optimizer,loss_f, train_loader, val_loader, epochs_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIz8zQMDydTw",
    "outputId": "d8c6926d-2f98-4ebe-e3c4-a9af7434ba86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-30 19:53:55,839]\u001b[0m A new study created in memory with name: no-name-0bea7114-f75f-48ae-93fc-06303b3db4a8\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 19:54:37,792]\u001b[0m Trial 0 finished with value: 0.8837316176470588 and parameters: {'embedding_space_dim': 80, 'learning_rate': 0.001, 'weight_decay': 0.001}. Best is trial 0 with value: 0.8837316176470588.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 19:55:12,721]\u001b[0m Trial 1 finished with value: 0.8720894607843137 and parameters: {'embedding_space_dim': 60, 'learning_rate': 0.001, 'weight_decay': 0.001}. Best is trial 0 with value: 0.8837316176470588.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 19:56:05,378]\u001b[0m Trial 2 finished with value: 0.8731107026143792 and parameters: {'embedding_space_dim': 100, 'learning_rate': 0.001, 'weight_decay': 0.005}. Best is trial 0 with value: 0.8837316176470588.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 19:57:06,075]\u001b[0m Trial 3 finished with value: 0.8420649509803921 and parameters: {'embedding_space_dim': 120, 'learning_rate': 0.01, 'weight_decay': 0.01}. Best is trial 0 with value: 0.8837316176470588.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 19:58:29,945]\u001b[0m Trial 4 finished with value: 0.8373672385620915 and parameters: {'embedding_space_dim': 160, 'learning_rate': 0.001, 'weight_decay': 0.01}. Best is trial 0 with value: 0.8837316176470588.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 19:59:11,209]\u001b[0m Trial 5 finished with value: 0.9103349673202614 and parameters: {'embedding_space_dim': 80, 'learning_rate': 0.01, 'weight_decay': 0.001}. Best is trial 5 with value: 0.9103349673202614.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 19:59:52,856]\u001b[0m Trial 6 finished with value: 0.8645322712418301 and parameters: {'embedding_space_dim': 80, 'learning_rate': 0.005, 'weight_decay': 0.005}. Best is trial 5 with value: 0.9103349673202614.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:01:04,851]\u001b[0m Trial 7 finished with value: 0.9083946078431373 and parameters: {'embedding_space_dim': 140, 'learning_rate': 0.01, 'weight_decay': 0.001}. Best is trial 5 with value: 0.9103349673202614.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:01:38,721]\u001b[0m Trial 8 finished with value: 0.8716809640522876 and parameters: {'embedding_space_dim': 60, 'learning_rate': 0.005, 'weight_decay': 0.005}. Best is trial 5 with value: 0.9103349673202614.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:01:38,721]\u001b[0m Trial 9 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:01:39,174]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:01:39,189]\u001b[0m Trial 11 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:01:39,194]\u001b[0m Trial 12 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:01:39,199]\u001b[0m Trial 13 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:01:39,204]\u001b[0m Trial 14 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:02:39,777]\u001b[0m Trial 15 finished with value: 0.9088031045751634 and parameters: {'embedding_space_dim': 120, 'learning_rate': 0.01, 'weight_decay': 0.001}. Best is trial 5 with value: 0.9103349673202614.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:02:39,788]\u001b[0m Trial 16 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:03:40,374]\u001b[0m Trial 17 finished with value: 0.9131433823529412 and parameters: {'embedding_space_dim': 120, 'learning_rate': 0.005, 'weight_decay': 0.001}. Best is trial 17 with value: 0.9131433823529412.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:03:40,828]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:04:33,105]\u001b[0m Trial 19 finished with value: 0.878421160130719 and parameters: {'embedding_space_dim': 100, 'learning_rate': 0.005, 'weight_decay': 0.005}. Best is trial 17 with value: 0.9131433823529412.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:04:33,112]\u001b[0m Trial 20 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:04:33,118]\u001b[0m Trial 21 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:04:33,125]\u001b[0m Trial 22 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:04:33,131]\u001b[0m Trial 23 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:04:33,139]\u001b[0m Trial 24 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:04:33,146]\u001b[0m Trial 25 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:05:14,381]\u001b[0m Trial 26 finished with value: 0.9121732026143791 and parameters: {'embedding_space_dim': 80, 'learning_rate': 0.005, 'weight_decay': 0.001}. Best is trial 17 with value: 0.9131433823529412.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:05:14,381]\u001b[0m Trial 27 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:05:14,397]\u001b[0m Trial 28 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:05:15,078]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:05:15,093]\u001b[0m Trial 30 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:05:15,101]\u001b[0m Trial 31 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:05:15,107]\u001b[0m Trial 32 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:06:07,476]\u001b[0m Trial 33 finished with value: 0.9147773692810457 and parameters: {'embedding_space_dim': 100, 'learning_rate': 0.01, 'weight_decay': 0.001}. Best is trial 33 with value: 0.9147773692810457.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:06:08,391]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:06:08,398]\u001b[0m Trial 35 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:07:00,998]\u001b[0m Trial 36 finished with value: 0.909671160130719 and parameters: {'embedding_space_dim': 100, 'learning_rate': 0.005, 'weight_decay': 0.001}. Best is trial 33 with value: 0.9147773692810457.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:07:01,705]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:07:07,229]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-30 20:07:07,244]\u001b[0m Trial 39 pruned. Duplicate Parameter Set\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction = \"maximize\")\n",
    "study.optimize(objective,n_trials = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PemzUqDYJEae"
   },
   "source": [
    "After the training, we pick the model that performed best on the validation set and test it on the Test Set. Obviously no hyperparameters have been chosen by looking on the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TJQcY35_K2w0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST HYPERPARAMETERS:\n",
      "Embedding Space Dimension = 100\n",
      "Learning Rate = 0.01\n",
      "Weight Decay = 0.001\n",
      "Best model final Test loss: 0.8830820540935672\n",
      "Best model final Test accuracy: 0.3064073716600736\n"
     ]
    }
   ],
   "source": [
    "pruned_trials = study.get_trials(deepcopy = False, states = [optuna.trial.TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy = False, states = [optuna.trial.TrialState.COMPLETE])\n",
    "\n",
    "# Extract the best hyperparameters \n",
    "best_parameters = study.best_params\n",
    "embedding_space_dim = best_parameters[\"embedding_space_dim\"]\n",
    "learning_rate = best_parameters[\"learning_rate\"]\n",
    "weight_decay = best_parameters[\"weight_decay\"]\n",
    "\n",
    "# Initialize Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize Model with best Hyperparameters\n",
    "num_node_features = train_dataset.num_features\n",
    "num_graph_features = train_dataset.g_features\n",
    "best_model = GNN(num_n_feature = num_node_features, num_g_feature = num_graph_features, emb_size = embedding_space_dim).to(device)\n",
    "best_optimizer = torch.optim.Adam(best_model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "loss_f = torch.nn.NLLLoss()\n",
    "\n",
    "\n",
    "\n",
    "# Train the Final Model\n",
    "acc_losses_train, acc_losses_val, acc_losses_test = [], [], []\n",
    "epochs = 60\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  loss_train, acc_train = train_step(best_model, train_loader, best_optimizer, loss_f)\n",
    "  loss_val, acc_val = val_step(best_model, val_loader, loss_f)\n",
    "  loss_test, acc_test = val_step(best_model, test_loader, loss_f)\n",
    "        \n",
    "  acc_losses_train.append([loss_train, acc_train])\n",
    "  acc_losses_val.append([loss_val, acc_val])\n",
    "  acc_losses_test.append([loss_test, acc_test])\n",
    "\n",
    "\n",
    "print(f\"BEST HYPERPARAMETERS:\")\n",
    "print(f\"Embedding Space Dimension = {embedding_space_dim}\")\n",
    "print(f\"Learning Rate = {learning_rate}\")\n",
    "print(f\"Weight Decay = {weight_decay}\")\n",
    "\n",
    "print(f\"Best model final Test loss: {acc_losses_test[-1][1]}\")\n",
    "print(f\"Best model final Test accuracy: {acc_losses_test[-1][0]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vzUTHQZK3SB"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "history_visible": true,
   "name": "Layout_Model_Hyperparameter_Optimization_Optuna.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
