{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEmsKFyyGxHT"
   },
   "source": [
    "# **Basic GNN Model Hyperparameter Optimization**\n",
    "\n",
    "This notebook search for the best hyperparameters by implementing a Cross-Validation Setup. The Hyperparameters that will be searched are the Learning Rate, the Weight Decay and the Embedding Size of the Graph Neural Network.\n",
    "An Early Stopping Mechanism will help terminate trials that are not improving.\n",
    "\n",
    "The data from the UPFD Framework has been already split in Training, Validation and Test Set and is downloadable by using simple commands inside the Pytorch Geometric environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHmjRogXHXhz"
   },
   "source": [
    "## Download and import Libraries for the Environment \n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDPul_f5Lqfg"
   },
   "source": [
    "Download the right libraries depending if we are using a CPU or a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIWCzgikGwbH",
    "outputId": "11220e1d-b290-4d22-f788-c388408f2c3d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "  #!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.10.0+cu102.html\n",
    "#else:\n",
    "  #!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.10.0+cpu.html\n",
    "\n",
    "#!pip install optuna\n",
    "import optuna\n",
    "\n",
    "\n",
    "from torch.nn import Linear, LogSoftmax\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool, global_max_pool\n",
    "\n",
    "from tool_box.upfd_dataset import ext_UPFD\n",
    "from tool_box.GNN_train import optimize_GNN, train_step, val_step\n",
    "\n",
    "# Set GPU as Device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(f\"Training Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ8KZjaYSBKP"
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M9OEYmMfRwPP"
   },
   "outputs": [],
   "source": [
    "path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAKBZdnB9Hkt",
    "outputId": "a5f30164-46d6-4a57-dfbd-3c0d22c57f41"
   },
   "outputs": [],
   "source": [
    "train_dataset = ext_UPFD(name = 'original', root = path, n_features=[], g_features=[], split = 'train')\n",
    "val_dataset = ext_UPFD(name = 'original', root = path, n_features=[], g_features=[], split='val')\n",
    "test_dataset = ext_UPFD(name = 'original', root = path, n_features=[], g_features=[], split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gcpp_2zetD1x"
   },
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lCtlg09CtXVY"
   },
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_n_feature, num_g_feature, emb_size):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        self.in_layer = GCNConv(num_n_feature, emb_size)\n",
    "        self.conv1 = GCNConv(emb_size, emb_size)\n",
    "       \n",
    "       # check if we have graph features to concatenate or not\n",
    "        i = 2\n",
    "        if num_g_feature:\n",
    "             self.lin_g = Linear(num_g_feature, emb_size)\n",
    "             i = 3\n",
    "\n",
    "        self.out_layer = Linear(i * emb_size, 2)\n",
    "        self.act = LogSoftmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x, edges_idx, batch_idx, g_features):\n",
    "        #pdb.set_trace()\n",
    "        x = self.in_layer(x, edges_idx)\n",
    "\n",
    "        x = self.conv1(x, edges_idx)\n",
    "\n",
    "        flatten = torch.cat([global_mean_pool(x, batch_idx),\n",
    "                             global_max_pool(x, batch_idx)], axis=1)\n",
    "\n",
    "        if g_features.size()[-1]:\n",
    "            g_ft = self.lin_g(g_features)\n",
    "            flatten = torch.cat([flatten, g_ft], axis=1)\n",
    "\n",
    "        out = self.act(self.out_layer(flatten))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy1kq27yuTh1"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ_D2AuKU-pa"
   },
   "source": [
    "Training Function that will be executed every time a trial is done. In this function the hyperparameters are chosen based on suggestions from the hyperparameter optimizer chosen with Optuna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gcRszH5TuZcI"
   },
   "outputs": [],
   "source": [
    "epochs_max = 60     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIz8zQMDydTw",
    "outputId": "d8c6926d-2f98-4ebe-e3c4-a9af7434ba86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-31 16:28:00,187]\u001b[0m A new study created in memory with name: no-name-ad52a69e-f599-4e8c-b23d-388d9e8c5c4c\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:31:07,989]\u001b[0m Trial 0 finished with value: 0.8728170955882353 and parameters: {'learning_rate': 0.005, 'weight_decay': 0.001, 'batch_size': 512, 'embedding_space_dim': 160}. Best is trial 0 with value: 0.8728170955882353.\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:31:50,217]\u001b[0m Trial 1 finished with value: 0.8148488562091504 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.01, 'batch_size': 64, 'embedding_space_dim': 40}. Best is trial 0 with value: 0.8728170955882353.\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:34:08,223]\u001b[0m Trial 2 finished with value: 0.8434627757352942 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.005, 'batch_size': 512, 'embedding_space_dim': 140}. Best is trial 0 with value: 0.8728170955882353.\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:35:57,566]\u001b[0m Trial 3 finished with value: 0.8742851307189542 and parameters: {'learning_rate': 0.005, 'weight_decay': 0.001, 'batch_size': 64, 'embedding_space_dim': 140}. Best is trial 3 with value: 0.8742851307189542.\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:36:49,032]\u001b[0m Trial 4 finished with value: 0.8714154411764706 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.001, 'batch_size': 128, 'embedding_space_dim': 60}. Best is trial 3 with value: 0.8742851307189542.\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:36:56,816]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:36:57,508]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:36:58,398]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:36:59,391]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:37:51,157]\u001b[0m Trial 9 finished with value: 0.8833486519607843 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.001, 'batch_size': 256, 'embedding_space_dim': 60}. Best is trial 9 with value: 0.8833486519607843.\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:39:11,188]\u001b[0m Trial 10 finished with value: 0.8937653186274509 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.001, 'batch_size': 256, 'embedding_space_dim': 100}. Best is trial 10 with value: 0.8937653186274509.\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:39:11,210]\u001b[0m Trial 11 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:39:11,226]\u001b[0m Trial 12 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:39:19,429]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:39:19,442]\u001b[0m Trial 14 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:39:21,034]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:39:24,181]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:39:31,745]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:39:33,079]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:39:33,923]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:40:51,865]\u001b[0m Trial 20 finished with value: 0.9022977941176471 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.001, 'batch_size': 128, 'embedding_space_dim': 100}. Best is trial 20 with value: 0.9022977941176471.\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:40:51,884]\u001b[0m Trial 21 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:40:51,896]\u001b[0m Trial 22 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:40:51,917]\u001b[0m Trial 23 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:40:52,794]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:40:52,808]\u001b[0m Trial 25 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:40,676]\u001b[0m Trial 26 finished with value: 0.905560661764706 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.001, 'batch_size': 256, 'embedding_space_dim': 140}. Best is trial 26 with value: 0.905560661764706.\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:45,915]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:48,326]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:51,450]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:53,238]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:54,750]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:55,501]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:55,525]\u001b[0m Trial 33 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:56,550]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:58,556]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:59,396]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:42:59,409]\u001b[0m Trial 37 pruned. Duplicate Parameter Set\u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:43:01,690]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-12-31 16:43:30,262]\u001b[0m Trial 39 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optimize_GNN(GNN, train_dataset, val_dataset, num_trials = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PemzUqDYJEae"
   },
   "source": [
    "After the training, we pick the model that performed best on the validation set and test it on the Test Set. Obviously no hyperparameters have been chosen by looking on the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TJQcY35_K2w0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST HYPERPARAMETERS:\n",
      "Embedding Space Dimension = 140\n",
      "Learning Rate = 0.01\n",
      "Weight Decay = 0.001\n",
      "Batch Size = 256\n",
      "Best model final Test loss: 0.8877044593663912\n",
      "Best model final Test accuracy: 0.30249659419059755\n"
     ]
    }
   ],
   "source": [
    "pruned_trials = study.get_trials(deepcopy = False, states = [optuna.trial.TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy = False, states = [optuna.trial.TrialState.COMPLETE])\n",
    "\n",
    "# Extract the best hyperparameters \n",
    "best_parameters = study.best_params\n",
    "embedding_space_dim = best_parameters[\"embedding_space_dim\"]\n",
    "learning_rate = best_parameters[\"learning_rate\"]\n",
    "weight_decay = best_parameters[\"weight_decay\"]\n",
    "batch_size = best_parameters[\"batch_size\"]\n",
    "\n",
    "# Initialize Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize Model with best Hyperparameters\n",
    "num_node_features = train_dataset.num_features\n",
    "num_graph_features = train_dataset.g_features\n",
    "best_model = GNN(num_n_feature = num_node_features, num_g_feature = num_graph_features, emb_size = embedding_space_dim).to(device)\n",
    "best_optimizer = torch.optim.Adam(best_model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "loss_f = torch.nn.NLLLoss()\n",
    "\n",
    "# Train the Final Model\n",
    "acc_losses_train, acc_losses_val, acc_losses_test = [], [], []\n",
    "\n",
    "for epoch in range(epochs_max):\n",
    "  loss_train, acc_train = train_step(best_model, train_loader, best_optimizer, loss_f)\n",
    "  loss_val, acc_val = val_step(best_model, val_loader, loss_f)\n",
    "  loss_test, acc_test = val_step(best_model, test_loader, loss_f)\n",
    "        \n",
    "  acc_losses_train.append([loss_train, acc_train])\n",
    "  acc_losses_val.append([loss_val, acc_val])\n",
    "  acc_losses_test.append([loss_test, acc_test])\n",
    "\n",
    "\n",
    "print(f\"BEST HYPERPARAMETERS:\")\n",
    "print(f\"Embedding Space Dimension = {embedding_space_dim}\")\n",
    "print(f\"Learning Rate = {learning_rate}\")\n",
    "print(f\"Weight Decay = {weight_decay}\")\n",
    "print(f\"Batch Size = {batch_size}\")\n",
    "\n",
    "print(f\"Best model final Test loss: {acc_losses_test[-1][1]}\")\n",
    "print(f\"Best model final Test accuracy: {acc_losses_test[-1][0]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vzUTHQZK3SB"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "history_visible": true,
   "name": "Layout_Model_Hyperparameter_Optimization_Optuna.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
